---
title: "p8105 Homework 2"
author: Rebekah Hughes
output: github_document
---

In the code chunk below, all the packages used in this homework are loaded.

```{r}
library(tidyverse)
library(readxl)
```


## Problem 1

The following code reads and cleans the Mr. Trash Wheel dataset.

```{r}
mrtrash_df = 
  read_excel("./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
  sheet = "Mr. Trash Wheel",
  range = "A2:N408") %>%
  janitor::clean_names() %>%
  drop_na (dumpster) %>% 
  mutate (
    sports_balls = round (sports_balls, digits = 0), 
    sports_balls = as.integer(sports_balls))
```

The following code reads and cleans the Precipitation data for 2017 and 2018.

```{r}
precip_2017 =
  read_excel("./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
             sheet = "2017 Precipitation",
             range = "A2:B14") %>% 
  mutate(Year = 2017) %>% 
  relocate(Year)

precip_2018 =
  read_excel("./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
             sheet = "2018 Precipitation",
             range = "A2:B14") %>% 
  mutate(Year = 2018) %>% 
  relocate(Year)
```

The next code chunk changes the month number to a month name variable and combines the precipitation datasets together.

```{r}
month_df =
  tibble(
    month = 1:12,
    month_name = month.name
  )

precip_tidy =
  bind_rows(precip_2017, precip_2018) %>% 
  janitor::clean_names()

precip_final =
  left_join(precip_tidy, month_df, by = "month")
```


The above datasets used for problem 1 have the following numbers of observations: `r nrow(mrtrash_df)` for the Mr. Trash Wheel dataset; `r nrow(precip_2017)` for the 2017 Precipitation dataset; `r nrow(precip_2018)` for the 2018 Precipitation dataset; and `r nrow(precip_final)` for the combined precipitation dataset. The following are examples of key variables from the Mr. Trash Wheel dataset: the mean of weight is `r mean (mrtrash_df$weight_tons)` tons, with a maximum value of `r max(mrtrash_df$weight_tons)` tons and a minimum value of `r min(mrtrash_df$weight_tons)` tons; the mean volume is `r mean(mrtrash_df$volume_cubic_yards)`, with a maximum of `r max(mrtrash_df$volume_cubic_yards)` and a minimum of `r min(mrtrash_df$volume_cubic_yards)`; and the mean number of homes powered is `r mean(mrtrash_df$homes_powered`. The following are examples of key variables from the combined precipitation dataset and the two individual precipitation datasets: the mean total rainfall across the two years is `r mean(precip_final$total)` inches; the mean total rainfall for 2017 alone is `r mean(precip_2017$Total)` inches; and the mean total rainfall for 2018 is `r mean(precip_2018$Total)` inches. The total precipitation in 2018 was `r sum (precip_2018$Total)` and the median number of sports balls in a dumpster in 2017 was `r median (mrtrash_df$sports_balls, "year" == 2017)`.


## Problem 2

```{r}
nycsub_df = 
  read_csv ("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>%
  select(
    line, station_name, station_latitude, station_longitude, route1, route2, route3,route4,
    route5, route6, route7, route8, route9, route10, route11, entry, vending, entrance_type,
    ada)%>%
  mutate(entry = recode(entry, YES = TRUE, NO = FALSE))
```

The above dataset contains the line, station name, routes 1-11, vending and entrance type as character variables, station latitude and longitude as nunerical variables, and ADA compliance and the recoded entry variable as logical variables. So far to clean the data, clean names has been done, certain variables have been cut out of the dataset, and entry has been recoded as a logical variable. The dimensions of the dataset are `r nrow(nycsub_df)` rows by `r ncol(nycsub_df)` columns. The data is more tidy than what it was initally but could be made more tidy still.

The following code makes station name a distinct variable.

```{r}
nycsub_dist =
  nycsub_df %>% 
  distinct(line, station_name) %>% 
  count()
```

Using the above code, it can be determined that there are 465 distinct stations.

The following code computes how many stations are ADA compliant.

```{r}
nycada_df =
  nycsub_df %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(ada == TRUE) %>% 
  count()
```

According to the output of the above code, there are 84 stations that are ADA compliant.

The following code calculates the proportion of station entrances/exits without vending that allow entrance.

```{r}
nycsub_filt = 
  nycsub_df %>%
  filter(vending == "NO", entry == TRUE) %>%
  mutate(proportion = entry / sum(entry))
```

According to the above code, the proportion of station entrances/exits without vending that allow entrance is about 0.0145 or 1.45%.

The following code reformats route number and route name as distinct variables.

```{r}
nycsub_tidy =
  nycsub_df %>%
  mutate(route8 = as.character(route8), route9 = as.character(route9), route10 = as.character(route10), route11 = as.character(route11)) %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_name",
    names_prefix = "route",
    values_to = "route_number"
  )

nyc_sub_a =
  nycsub_tidy %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(route_number == "A") %>% 
  count()
```

From the above code, it can be determined that 60 distinct stations serve the A train.

```{r}
nyc_sub_a_ada =
  nycsub_tidy %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(route_number == "A", ada == TRUE) %>% 
  count()
```

Given the above code, it can be concluded that of the stations that serve the A train, only 17 are ADA compliant.


## Problem 3

The following code chunk reads and cleans the National Politicians dataset.

```{r}
pols_month_df =
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>%
  janitor::clean_names() %>%
  separate(mon, c("year", "month", "day")) %>%
  pivot_longer(
    cols = starts_with("prez"),
    names_to = "president",
    names_prefix = "prez_") %>%
    select(-day)

month_df1 =
  tibble(
    month = 01:12,
    month_name = month.name
  )

pols_final =
  left_join(pols_month_df, month_df1, by = "month")
```

The next code chunk reads and cleans the S&P dataset.

```{r}
sp_df =
  read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% 
  separate(date, c("month", "day", "year")) %>% 
  select(-day) %>% 
  arrange(year, month)
```

The following code chunk reads and tidies the Unemployment dataset.

```{r}
unemploy_df =
  read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% 
  mutate(year = as.character(year)) %>% 
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemploy_prop"
  )
```

The next bit of code joins the above three datasets together to form the final dataset.

```{r}
fivethirtyeight_df =
  left_join(pols_month_df, sp_df, by = "year") %>% 
  left_join(unemploy_df, by = "year") %>% 
  arrange(year)
```

The National Politician dataset contains data on the politicians in the US that are republican and democrat and contains the variables year, month, presidential party, and parties of senators and governors. The S&P dataset contains data regarding S&P's stock market index and contains the variables year, month and closing value. The unemployment dataset contains data on the percent of unemployment per month and contains the variables month, year and percent unemployed. The final joined dataset contains all the values of the three datasets but combines the years and months of each of the datasets. There are `r nrow(fivethirtyeight_df)` rows and `r ncol(fivethirtyeight_df)` columns in the final dataset. The range of years is from `r min(fivethirtyeight_df$year)` to `r max(fivethirtyeight_df$year)`. Key variables to note in the final dataset are the year and month, the presidential party in office, the unemployment percentages, and the closing value of the S&P index as an estimation of what the stock market looked like in general.